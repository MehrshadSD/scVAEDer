{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5f9e66e",
   "metadata": {},
   "source": [
    "# scVAEDer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.helper import sample_batch,extract,make_beta_schedule\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_fin= pd.read_csv(\"clust.csv\", index_col=0)\n",
    "data = Data_fin.loc[:, Data_fin.columns != 'clusters']\n",
    "Y= Data_fin['clusters']\n",
    "classes, counts = np.unique(Y, return_counts=True)\n",
    "label_encoder = LabelEncoder()\n",
    "numeric_labels = label_encoder.fit_transform(Data_fin[\"clusters\"])\n",
    "data = data.to_numpy(dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ccbfd",
   "metadata": {},
   "source": [
    "After reading the data and preprocessing steps now we divide the data in training and test sets (You can add validation set here too if you want to find the best architecture for your model). There are alot of ways to divide the data in order to train a machine learning model (for more information please check:https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/. Thank you Jason!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:1100]\n",
    "test_data = data[1100:]\n",
    "\n",
    "# Convert data to PyTorch tensors and create data loaders\n",
    "train_data = torch.Tensor(train_data)\n",
    "test_data = torch.Tensor(test_data)\n",
    "train_loader = DataLoader(TensorDataset(train_data), batch_size=200, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_data), batch_size=200, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94e859",
   "metadata": {},
   "source": [
    "Lets build our autoencoder (AE). You can Choose different types of AE. In our paper we used VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "laten_size=30   #Size of the latent layer\n",
    "layer1=100   #Size of the first layer of enocder and decoder\n",
    "input_size=1845\n",
    "\n",
    "# Define the variational autoencoder model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, layer1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer1, laten_size),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(laten_size, layer1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(layer1, input_size),\n",
    "        )\n",
    "        self.mu = nn.Linear(laten_size, laten_size)\n",
    "        self.log_var = nn.Linear(laten_size, laten_size)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.log_var(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x = self.decoder(z)\n",
    "        return x, mu, log_var\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        z = torch.randn(num_samples, laten_size)\n",
    "        return self.decoder(z)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function: We also tried different loss functions Binary Cross-Entropy (BCE) Loss for \n",
    "# the reconstuciton loss of VAE but MSE worked better (lower error):\n",
    "def vae_loss(x, x_recon, mu, log_var):\n",
    "    recon_loss = nn.functional.mse_loss(x_recon, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Train the model\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "num_epochs = 150\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = batch[0]\n",
    "        x_recon, mu, log_var = model(x)\n",
    "        loss = vae_loss(x, x_recon, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_data))\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch[0]\n",
    "            x_recon, mu, log_var = model(x)\n",
    "            loss = vae_loss(x, x_recon, mu, log_var)\n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss / len(test_data))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Convert the data to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data).float()\n",
    "\n",
    "# Compute the latent layer for the data\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    latent_layer = model.encoder(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f502dac",
   "metadata": {},
   "source": [
    "We can now check how the latent layer looks by using UMAP or tSNE (you can change the parameter values of UMAP to get a nicer plot). We also found doing a batch normalization of the latent layer helps the training process of DDM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13bdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_layer = (latent_layer - latent_layer.mean())/(latent_layer.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the latent layer using UMAP\n",
    "umap_embedding = umap.UMAP(n_neighbors=100, min_dist=0.3, random_state=42).fit_transform(latent_layer)\n",
    "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=numeric_labels, s=5, cmap='viridis')\n",
    "plt.show()\n",
    "dataset = torch.Tensor(latent_layer).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decffcc4",
   "metadata": {},
   "source": [
    "Now we want to train the DDM. First as described in the method section we define different types of Scheduler (more detail can be found here:https://github.com/acids-ircam/diffusion_models). ***IMPORTANT: I FOUND THE NUMBER OF STEPS (num_steps) EXTREMELY IMPORTANT IN THE QUALITY OF GENERATED SAMPLES SO PLEASE TRY DIFFERENT VALUS (hyper-parameter search):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_beta_schedule(schedule='linear', num_steps=1000, start=1e-5, end=1e-2):\n",
    "    if schedule == 'linear':\n",
    "        betas = torch.linspace(start, end, num_steps)\n",
    "    elif schedule == \"quad\":\n",
    "        betas = torch.linspace(start ** 0.5, end ** 0.5, num_steps) ** 2\n",
    "    elif schedule == \"sigmoid\":\n",
    "        betas = torch.linspace(-6, 6, num_steps)\n",
    "        betas = torch.sigmoid(betas) * (end - start) + start\n",
    "    return betas\n",
    "\n",
    "def extract(input, t, x):\n",
    "    shape = x.shape\n",
    "    out = torch.gather(input, 0, t.to(input.device))\n",
    "    reshape = [t.shape[0]] + [1] * (len(shape) - 1)\n",
    "    return out.reshape(*reshape)\n",
    "\n",
    "def plot_schedule(num_steps,schedule):\n",
    "    plt.plot(list(range(num_steps)),betas.numpy(),label='betas')\n",
    "    plt.plot(list(range(num_steps)),torch.sqrt(alphas_prod).numpy(),label='sqrt_alphas_prod')\n",
    "    plt.plot(list(range(num_steps)),torch.sqrt(1-alphas_prod).numpy(),label='sqrt_one_minus_alphas_prod')\n",
    "    plt.legend(['betas','sqrt_alphas_prod','sqrt_one_minus_alphas_prod'],loc = 'upper left')\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('value')\n",
    "    plt.title('{} schedule'.format(schedule))\n",
    "    plt.show()\n",
    "\n",
    "num_steps=2000\n",
    "\n",
    "schedule='sigmoid'\n",
    "betas = make_beta_schedule(schedule=schedule, num_steps=num_steps, start=1e-5, end=1e-2)\n",
    "alphas = 1-betas\n",
    "alphas_prod = torch.cumprod(alphas,0)\n",
    "alphas_prod_p = torch.cat([torch.tensor([1]).float(),alphas_prod[:-1]],0)\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "one_minus_alphas_bar_log = torch.log(1 - alphas_prod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)\n",
    "plot_schedule(num_steps,schedule)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a5b4d",
   "metadata": {},
   "source": [
    "Calculate the sample value x_t at any time, based on x_0 and reparameterization trick. You can compute x(t) time based on the initial x_0 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d0b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_x(x_0,t):\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "    alphas_t = alphas_bar_sqrt[t].to(device)\n",
    "    alphas_1_m_t = one_minus_alphas_bar_sqrt[t].to(device)\n",
    "    return (alphas_t * x_0 + alphas_1_m_t * noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c9ebcc",
   "metadata": {},
   "source": [
    "Run the code below to check the forward process. (Its not necessary therefore I commented it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58aa9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_shows = 20\n",
    "#fig,axs = plt.subplots(2,10,figsize=(20,5))\n",
    "#plt.rc('text',color='black')\n",
    "\n",
    "#for i in range(num_shows):\n",
    "    #print(i)\n",
    "    #j = i//10\n",
    "    #k = i%10\n",
    "    #q_i = q_x(dataset,torch.tensor([i*num_steps//num_shows]))#Generate sample data at time t\n",
    "    #umap_emb = umap.UMAP(n_neighbors=80, min_dist=0.3).fit_transform(q_i)\n",
    "    #axs[j,k].scatter(umap_emb[:, 0], umap_emb[:, 1], s=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4437cce",
   "metadata": {},
   "source": [
    "As can be seen from the figure above adding random noise to the initial data gradually affects its integrity. Throughout the forward diffusion process, noise is systematically added, resulting in the generation of noisy data. To acquire an understanding of the iteratively added noise during the forward process, a neural network model is trained. The objective is to learn the patterns and characteristics of the noise introduced at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6caa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDiffusion(nn.Module):\n",
    "    def __init__(self,n_steps, num_units=512):\n",
    "        super(MLPDiffusion,self).__init__()\n",
    "        \n",
    "        self.linears = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(laten_size,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,laten_size),\n",
    "            ]\n",
    "        )\n",
    "        self.step_embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "            ]\n",
    "        )\n",
    "    def forward(self,x,t):\n",
    "#         x = x_0\n",
    "        for idx,embedding_layer in enumerate(self.step_embeddings):\n",
    "            t_embedding = embedding_layer(t)\n",
    "            x = self.linears[2*idx](x)\n",
    "            x += t_embedding\n",
    "            x = self.linears[2*idx+1](x)\n",
    "            \n",
    "        x = self.linears[-1](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d2083",
   "metadata": {},
   "source": [
    "We define the error function for training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_loss_fn(model, x_0, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, n_steps):\n",
    "    \"\"\"Sampling and calculating loss at any time t\"\"\"\n",
    "    batch_size = x_0.shape[0]\n",
    "    \n",
    "    ##Generate random time t for a batchsize sample\n",
    "    t = torch.randint(0,n_steps,size=(batch_size//2,), device=x_0.device)\n",
    "    t = torch.cat([t,n_steps-1-t],dim=0)\n",
    "    t = t.unsqueeze(-1)\n",
    "    \n",
    "    #Coefficient of x0\n",
    "    a = alphas_bar_sqrt[t]\n",
    "    \n",
    "    #Coefficient of eps\n",
    "    aml = one_minus_alphas_bar_sqrt[t]\n",
    "    \n",
    "    #generate random noise eps\n",
    "    e = torch.randn_like(x_0, device=x_0.device)\n",
    "    \n",
    "    #Construct the input of the model\n",
    "    x = (x_0*a+e*aml).to(device)\n",
    "    \n",
    "    #Send into the model to get the predicted value of random noise at time t\n",
    "    output = model(x, t.squeeze(-1))\n",
    "    \n",
    "    #Compute error along with real noise, average\n",
    "    return (e - output).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0702514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample_loop(model, shape, n_steps, betas, one_minus_alphas_bar_sqrt):\n",
    "    \"\"\"Restore x[T-1], x[T-2]|...x[0] from x[T]\"\"\"\n",
    "    cur_x = torch.randn(shape).to(device)\n",
    "    x_seq = [cur_x]\n",
    "    for i in reversed(range(n_steps)):\n",
    "        cur_x = p_sample(model, cur_x, i, betas, one_minus_alphas_bar_sqrt).to(device)\n",
    "        x_seq.append(cur_x)\n",
    "    return x_seq\n",
    "\n",
    "\n",
    "def p_sample(model, x, t, betas, one_minus_alphas_bar_sqrt):\n",
    "    \"\"\"Sampling the reconstructed value at time t from x[T]\"\"\"\n",
    "    device=\"cpu\"\n",
    "    t = torch.tensor([t]).to(device)\n",
    "    betas = betas.to(device)\n",
    "    one_minus_alphas_bar_sqrt = one_minus_alphas_bar_sqrt.to(device)\n",
    "    coeff = (betas[t] / one_minus_alphas_bar_sqrt[t]).to(device)\n",
    "\n",
    "    eps_theta = model(x, t).to(device)\n",
    "\n",
    "    mean = (1 / (1 - betas[t]).sqrt()) * (x - (coeff * eps_theta)).to(device)\n",
    "\n",
    "    z = torch.randn_like(x).to(device)\n",
    "    sigma_t = betas[t].sqrt().to(device)\n",
    "\n",
    "    sample = mean + sigma_t * z\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15843f",
   "metadata": {},
   "source": [
    "The trained model can be used to generate novel data. To accomplish this, we simply use Gaussian noise and the formula mentioned in the method section to perform sampling. It has been previously mentioned that Exponential Moving Average(EMA) can inmprove the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4bc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA():\n",
    "    def __init__(self,mu=0.001):\n",
    "        self.mu = mu\n",
    "        self.shadow = {}\n",
    "        \n",
    "    def register(self,name,val):\n",
    "        self.shadow[name] = val.clone()\n",
    "        \n",
    "    def __call__(self,name,x):\n",
    "        assert name in self.shadow\n",
    "        new_average = self.mu * x + (1.0-self.mu)*self.shadow[name]\n",
    "        self.shadow[name] = new_average.clone()\n",
    "        return new_average\n",
    "\n",
    "device=\"cuda\"\n",
    "total_loss=[]\n",
    "print('Training model...')\n",
    "batch_size = 1200\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "num_epoch = 3000\n",
    "plt.rc('text',color='blue')\n",
    "model2 = MLPDiffusion(num_steps)\n",
    "model2.to(device)\n",
    "model2 = torch.nn.DataParallel(model2).to(device)\n",
    "alphas_bar_sqrt = alphas_bar_sqrt.to(device)\n",
    "one_minus_alphas_bar_sqrt = one_minus_alphas_bar_sqrt.to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(),lr=1e-3)\n",
    "\n",
    "for t in range(num_epoch):\n",
    "    print(\"The values are: {} and {}\".format(loss, t))\n",
    "    for idx,batch_x in enumerate(dataloader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        loss = diffusion_loss_fn(model2, batch_x, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, num_steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model2.parameters(),1.)\n",
    "        optimizer.step()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a5d03",
   "metadata": {},
   "source": [
    "Now lets save the model and change the device from 'GPU' to 'CPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e115ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Specify a path to save to\n",
    "PATH = \"model_interpolate.pth\" # Choose whatever you like\n",
    "# Save\n",
    "torch.save(model2.state_dict(), PATH)\n",
    "# Load\n",
    "device = torch.device('cpu')\n",
    "model4 = MLPDiffusion(num_steps)\n",
    "\n",
    "# Original saved file with DataParallel\n",
    "state_dict = torch.load(\"model_interpolate.pth\")\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "model4.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0ef0bd",
   "metadata": {},
   "source": [
    "By using the sampling formula we can generate novel data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978f24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq = p_sample_loop(model4, dataset.shape, num_steps, betas, one_minus_alphas_bar_sqrt)\n",
    "len(x_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ea978",
   "metadata": {},
   "source": [
    "By selecting the desired number of denoising steps, the generated data can be visualized using UMAP. \n",
    "Keep in mind that increasing the number of steps leads to higher-quality results. For example I add noise in 1000 steps in the forward process then I will select 1000 steps for denoising (reverse process) and save the output in cur_x: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e9f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_x = x_seq[len(x_seq)-1].detach().cpu().numpy()\n",
    "umap_emb = umap.UMAP(n_neighbors=100, min_dist=0.2).fit_transform(cur_x)\n",
    "plt.scatter(umap_emb[:, 0], umap_emb[:, 1], s=10);  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c20a5",
   "metadata": {},
   "source": [
    "Now with the generated sample we can perfrom clustering (I chose Kmean you can try other clustering methods if you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "# Initialize KMeans clustering with 5 clusters\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "# Get the last tensor in X_seq\n",
    "last_tensor = x_seq[-1].detach().cpu()\n",
    "# Perform KMeans clustering on the tensor\n",
    "cluster_labels = kmeans.fit_predict(last_tensor)\n",
    "# Replot the UMAP with cells colored based on their cluster assignment\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "umap_emb = umap.UMAP(n_neighbors=15, min_dist=0.09).fit_transform(last_tensor.detach().cpu().numpy())\n",
    "scatter = ax.scatter(umap_emb[:, 0], umap_emb[:, 1], c=cluster_labels, cmap='viridis', s=10)\n",
    "plt.title('UMAP plot with cluster assignments')\n",
    "plt.show()\n",
    "\n",
    "# Print the total time taken to run the code\n",
    "print(f'Total time taken: {time.time() - start_time:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f5979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_sample(cell_type):\n",
    "    # create a Pandas dataframe\n",
    "    latent_layer_array = pd.DataFrame(latent_layer.numpy())\n",
    "    latent_layer_pd = pd.DataFrame(latent_layer_array)\n",
    "    latent_layer_pd.index= Data_fin.index\n",
    "    latent_layer_pd = latent_layer_pd.join(Data_fin['clusters'])\n",
    "    hspc_df = latent_layer_pd[latent_layer_pd['clusters'] == cell_type]\n",
    "    data2 = hspc_df.loc[:, hspc_df.columns != 'clusters']\n",
    "    return data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thromb = cell_sample(\"Thrombocytes\")\n",
    "hspc = cell_sample(\"HSPC\")\n",
    "Thromb_diff_latent = q_x(torch.tensor(thromb.iloc[:,:].values), torch.tensor([num_steps-1]))\n",
    "hspc_diff_latent = q_x(torch.tensor(hspc.iloc[:,:].values), torch.tensor([num_steps-1]))\n",
    "Thromb_diff_latent_m = torch.tensor(Thromb_diff_latent.mean(axis=0))\n",
    "hspc_diff_latent_m = torch.tensor(hspc_diff_latent.mean(axis=0))\n",
    "\n",
    "alpha = torch.tensor(np.linspace(0, 1, 2000, dtype=np.float32)).to(device)\n",
    "intp = Thromb_diff_latent_m* (1 - alpha[:, None]) + hspc_diff_latent_m * alpha[:, None]\n",
    "\n",
    "def p_sample_loop2(model, shape, n_steps, betas, one_minus_alphas_bar_sqrt):\n",
    "    cur_x = intp.to(device)\n",
    "    x_seq = [cur_x]\n",
    "    for i in reversed(range(n_steps)):\n",
    "        cur_x = p_sample(model, cur_x, i, betas, one_minus_alphas_bar_sqrt)\n",
    "        x_seq.append(cur_x)\n",
    "    return x_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5551b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "intp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq_zero = p_sample_loop2(model4, intp.shape, num_steps, betas, one_minus_alphas_bar_sqrt)\n",
    "cur_x2 = x_seq_zero[len(x_seq_zero)-1].detach().numpy()\n",
    "umap_emb2 = umap.UMAP(n_neighbors=10, min_dist=0.1).fit_transform(cur_x2)\n",
    "plt.scatter(umap_emb2[:, 0], umap_emb2[:, 1], s=10);\n",
    "\n",
    "with torch.no_grad():\n",
    "    decod_intrp = model.decoder(x_seq_zero[1500])\n",
    "column_names = list(Data_fin.columns) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
