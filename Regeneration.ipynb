{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c918102",
   "metadata": {},
   "source": [
    "If you dont mind lets look at another example using Biddy et al dataset. \n",
    "Here we want to inteprolate between failed to reprogrammed cells and detect master regulators in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import time\n",
    "import scanpy as sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292dc0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read('CellTagging_adata_preprocessed.h5ad')\n",
    "adata.obs.state_info.value_counts()\n",
    "sc.pp.filter_cells(adata, min_genes=100)\n",
    "sc.pp.filter_genes(adata, min_cells=200)\n",
    "sc.pp.normalize_per_cell(adata)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=2000)\n",
    "adata = adata[:, adata.var['highly_variable']]\n",
    "data=pd.DataFrame((adata.X))\n",
    "adata = adata[((adata.obs.state_info == \"Reprogrammed\") | (adata.obs.state_info == \"Failed\"))]\n",
    "data=pd.DataFrame(adata.X.todense())\n",
    "data = data.to_numpy(dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72bda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into train and test sets\n",
    "train_data = data[:3000]\n",
    "test_data = data[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc96c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "laten_size=45\n",
    "layer1=250\n",
    "genes_number=2000\n",
    "# Convert data to PyTorch tensors and create data loaders\n",
    "train_data = torch.Tensor(train_data)\n",
    "test_data = torch.Tensor(test_data)\n",
    "train_loader = DataLoader(TensorDataset(train_data), batch_size=500, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_data), batch_size=500, shuffle=True)\n",
    "\n",
    "# Define the variational autoencoder model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(genes_number, layer1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.15),\n",
    "            nn.Linear(layer1, laten_size),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(laten_size, layer1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layer1, genes_number),\n",
    "        )\n",
    "        self.mu = nn.Linear(laten_size, laten_size)\n",
    "        self.log_var = nn.Linear(laten_size, laten_size)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.log_var(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x = self.decoder(z)\n",
    "        return x, mu, log_var\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        z = torch.randn(num_samples, laten_size)\n",
    "        return self.decoder(z)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss function\n",
    "def vae_loss(x, x_recon, mu, log_var):\n",
    "    recon_loss = nn.functional.mse_loss(x_recon, x, reduction='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "# Train the model\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = batch[0]\n",
    "        x_recon, mu, log_var = model(x)\n",
    "        loss = vae_loss(x, x_recon, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_data))\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch[0]\n",
    "            x_recon, mu, log_var = model(x)\n",
    "            loss = vae_loss(x, x_recon, mu, log_var)\n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss / len(test_data))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Convert the data to a PyTorch tensor\n",
    "data_tensor = torch.tensor(data).float()\n",
    "\n",
    "# Compute the latent layer for the data\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    latent_layer = model.encoder(data_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db0b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "latent_layer = (latent_layer - latent_layer.mean())/(latent_layer.std())\n",
    "s_curve = latent_layer\n",
    "\n",
    "\n",
    "adata.obs.columns\n",
    "label_encoder = LabelEncoder()\n",
    "numeric_labels = label_encoder.fit_transform(adata.obs.state_info)\n",
    "import umap.umap_ as umap\n",
    "umap_embedding = umap.UMAP(n_neighbors=50, min_dist=0.8, random_state=42).fit_transform(latent_layer)\n",
    "plt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c=numeric_labels, s=5, cmap='viridis')\n",
    "plt.savefig(\"Good umap of failed reporogrammed.pdf\") \n",
    "plt.show()\n",
    "dataset = torch.Tensor(latent_layer).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320fdae9",
   "metadata": {},
   "source": [
    "Here I used the sigmoid function but you can choose other functions for variance scheduling. You can find them in the first Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000 \n",
    "#Develop the beta of each step\n",
    "betas = torch.linspace(-6,6,num_steps)\n",
    "betas = torch.sigmoid(betas)*(0.5e-2 - 1e-5) + 1e-5\n",
    "alphas = 1-betas\n",
    "alphas_prod = torch.cumprod(alphas,0)\n",
    "alphas_prod_p = torch.cat([torch.tensor([1]).float(),alphas_prod[:-1]],0)\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "one_minus_alphas_bar_log = torch.log(1 - alphas_prod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dbe58",
   "metadata": {},
   "source": [
    "Forward steps in DDM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_x(x_0,t):\n",
    "    \"\"\"You can get x[t] at any time t based on x[0]\"\"\"\n",
    "    noise = torch.randn_like(x_0)\n",
    "    alphas_t = alphas_bar_sqrt[t]\n",
    "    alphas_1_m_t = one_minus_alphas_bar_sqrt[t]\n",
    "    return (alphas_t * x_0 + alphas_1_m_t * noise) #Add noise based on x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb97453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose how many step you want to see\n",
    "num_shows = 20\n",
    "# choose your figure size\n",
    "fig,axs = plt.subplots(,,figsize=(20,5))\n",
    "for i in range(num_shows):\n",
    "    print(i)\n",
    "    j = i//10\n",
    "    k = i%10\n",
    "    q_i = q_x(dataset,torch.tensor([i*num_steps//num_shows]))      #Generate sample data at time t\n",
    "    umap_emb = umap.UMAP(n_neighbors=80, min_dist=0.3).fit_transform(q_i)\n",
    "    axs[j,k].scatter(umap_emb[:, 0], umap_emb[:, 1], c=numeric_labels, s=5, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7394b0d7",
   "metadata": {},
   "source": [
    "Neural network architecture to approximate the conditioned probability distributions in the reverse diffusion proces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDiffusion(nn.Module):\n",
    "    def __init__(self,n_steps, num_units=1200):\n",
    "        super(MLPDiffusion,self).__init__()\n",
    "        \n",
    "        self.linears = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(laten_size,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,num_units),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(num_units,laten_size),\n",
    "            ]\n",
    "        )\n",
    "        self.step_embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "                nn.Embedding(n_steps,num_units),\n",
    "            ]\n",
    "        )\n",
    "    def forward(self,x,t):\n",
    "#         x = x_0\n",
    "        for idx,embedding_layer in enumerate(self.step_embeddings):\n",
    "            t_embedding = embedding_layer(t)\n",
    "            x = self.linears[2*idx](x)\n",
    "            x += t_embedding\n",
    "            x = self.linears[2*idx+1](x)\n",
    "            \n",
    "        x = self.linears[-1](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee7b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_loss_fn(model,x_0,alphas_bar_sqrt,one_minus_alphas_bar_sqrt,n_steps):\n",
    "    batch_size = x_0.shape[0]\n",
    "    ##Generate random time t for a batchsize sample\n",
    "    t = torch.randint(0,n_steps,size=(batch_size//2,), device=x_0.device)\n",
    "    t = torch.cat([t,n_steps-1-t],dim=0)\n",
    "    t = t.unsqueeze(-1)\n",
    "    #Coefficient of x0\n",
    "    a = alphas_bar_sqrt[t]\n",
    "    #Coefficient of eps\n",
    "    aml = one_minus_alphas_bar_sqrt[t]\n",
    "    #Generate random noise eps\n",
    "    e = torch.randn_like(x_0, device=x_0.device)\n",
    "    #Construct the input of the model\n",
    "    x = (x_0*a+e*aml).to(device)\n",
    "    #Send into the model to get the predicted value of random noise at time t\n",
    "    output = model(x, t.squeeze(-1))\n",
    "    #Compute error along with real noise, average\n",
    "    return (e - output).square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample_loop(model, shape, n_steps, betas, one_minus_alphas_bar_sqrt):\n",
    "    \"\"\"Restore x[T-1], x[T-2]|...x[0] from x[T]\"\"\"\n",
    "    cur_x = torch.randn(shape).to(device)\n",
    "    x_seq = [cur_x]\n",
    "    for i in reversed(range(n_steps)):\n",
    "        cur_x = p_sample(model, cur_x, i, betas, one_minus_alphas_bar_sqrt).to(device)\n",
    "        x_seq.append(cur_x)\n",
    "    return x_seq\n",
    "\n",
    "def p_sample(model, x, t, betas, one_minus_alphas_bar_sqrt):\n",
    "    device=\"cpu\"\n",
    "    t = torch.tensor([t]).to(device)\n",
    "    betas = betas.to(device)\n",
    "    one_minus_alphas_bar_sqrt = one_minus_alphas_bar_sqrt.to(device)\n",
    "    coeff = (betas[t] / one_minus_alphas_bar_sqrt[t]).to(device)\n",
    "    eps_theta = model(x, t).to(device)\n",
    "    mean = (1 / (1 - betas[t]).sqrt()) * (x - (coeff * eps_theta)).to(device)\n",
    "    z = torch.randn_like(x).to(device)\n",
    "    sigma_t = betas[t].sqrt().to(device)\n",
    "    sample = mean + sigma_t * z\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA():\n",
    "    def __init__(self, mu=0.001):\n",
    "        self.mu = mu\n",
    "        self.shadow = {}\n",
    "    def register(self,name,val):\n",
    "        self.shadow[name] = val.clone()\n",
    "    def __call__(self,name,x):\n",
    "        assert name in self.shadow\n",
    "        new_average = self.mu * x + (1.0-self.mu)*self.shadow[name]\n",
    "        self.shadow[name] = new_average.clone()\n",
    "        return new_average\n",
    "\n",
    "device=\"cuda\"\n",
    "total_loss=[]\n",
    "print('Training model...')\n",
    "batch_size = 1000\n",
    "dataloader = torch.utils.data.DataLoader(dataset[:], batch_size=batch_size, shuffle=True)\n",
    "num_epoch = 1500\n",
    "plt.rc('text',color='blue')\n",
    "model2 = MLPDiffusion(num_steps) #The output dimension is 2, the input is x and step\n",
    "model2.to(device)\n",
    "model2 = torch.nn.DataParallel(model2).to(device)\n",
    "alphas_bar_sqrt = alphas_bar_sqrt.to(device)\n",
    "one_minus_alphas_bar_sqrt = one_minus_alphas_bar_sqrt.to(device)\n",
    "optimizer = torch.optim.Adam(model2.parameters(),lr=1e-3)\n",
    "\n",
    "for t in range(num_epoch):\n",
    "    print(\"The values are: {} and {}\".format(loss, t))\n",
    "    for idx,batch_x in enumerate(dataloader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        loss = diffusion_loss_fn(model2, batch_x, alphas_bar_sqrt, one_minus_alphas_bar_sqrt, num_steps)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model2.parameters(),1.)\n",
    "        optimizer.step()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"Morris.pth\"\n",
    "# Save\n",
    "torch.save(model2.state_dict(), PATH)\n",
    "# Load\n",
    "device = torch.device('cpu')\n",
    "model4 = MLPDiffusion(1000)\n",
    "\n",
    "# original saved file with DataParallel\n",
    "state_dict = torch.load(\"Morris.pth\")\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "# load params\n",
    "model4.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition=pd.DataFrame(adata.obs.state_info)\n",
    "condition=condition.reset_index()\n",
    "condition = condition.drop(condition.columns[0], axis=1)\n",
    "\n",
    "merged = pd.concat([condition, pd.DataFrame(latent_layer)],axis=1)\n",
    "reprog = merged[(merged.state_info == \"Reprogrammed\")]\n",
    "failed = merged[(merged.state_info == \"Failed\")]\n",
    "\n",
    "reprog_diff_latent = q_x(torch.tensor(reprog.iloc[:,1:].values), torch.tensor([num_steps-1]))\n",
    "failed_diff_latent = q_x(torch.tensor(failed.iloc[:,1:].values), torch.tensor([num_steps-1]))\n",
    "reprog_diff_latent_m=torch.tensor(reprog_diff_latent.mean(axis=0))\n",
    "failed_diff_latent_m=torch.tensor(failed_diff_latent.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0326ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_emb = umap.UMAP(n_neighbors=80, min_dist=0.3).fit_transform(failed_diff_latent)\n",
    "plt.scatter(umap_emb[:, 0], umap_emb[:, 1], s=5, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46db86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = torch.tensor(np.linspace(0, 1, 5000, dtype=np.float32)).to(device)\n",
    "intp = reprog_diff_latent_m* (1 - alpha[:, None]) + failed_diff_latent_m * alpha[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bada32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample_loop2(model, shape, n_steps, betas, one_minus_alphas_bar_sqrt):\n",
    "    \"\"\"Restore x[T-1], x[T-2]|...x[0] from x[T]\"\"\"\n",
    "    cur_x = intp.to(device)\n",
    "    x_seq = [cur_x]\n",
    "    for i in reversed(range(n_steps)):\n",
    "        cur_x = p_sample(model, cur_x, i, betas, one_minus_alphas_bar_sqrt)\n",
    "        x_seq.append(cur_x)\n",
    "    return x_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34813cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq_zero = p_sample_loop2(model4, intp.shape, num_steps, betas, one_minus_alphas_bar_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decod_intrp = model.decoder(x_seq_zero[1000])\n",
    "column_names = list(adata.var_names)  # List of column names from the \"data\" DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f484a31",
   "metadata": {},
   "source": [
    "You can ask me why are you computing velocity at all???? I believe looking at gene expression alone is not enough. \n",
    "In classical mechanics we have seen the benfit of using concpets such as velocity, momentum, acceleration, and jerk (rate at which an object's acceleration changes). I believe, when a model is capable of generating realistic and novel data, it indicates that it has learned the appropriate embedding. Also, Bengio et al. have demonstrated that causal models tend to adapt more quickly to interventions compared to predictive models. In this scenario, as we interpolate using a neural network, it is expected that key genes will undergo more rapid changes and exhibit higher velocity. I think it's not a bad idea to consider additional factors beyond gene expression, which might help us in better understanding the underlying phenomena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of top 30 genes with the highest velocity\n",
    "velocities = decod_intrp.diff()\n",
    "top_cols = velocities.apply(lambda x: x.nlargest(30).index.tolist(), axis=1)\n",
    "top_cols.to_csv(\"velocity_genes.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d628b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also compute the average velocity. Here I consider changes in values over each 10-step interval. \n",
    "# The resulting differences are then divided by 10 to obtain the velocity values.\n",
    "decod_intrp.index = pd.RangeIndex(start=0, stop=5000)\n",
    "decod_intrp.index = pd.to_datetime(decod_intrp.index, unit='s')\n",
    "df_resampled = decod_intrp.resample('10S').mean()\n",
    "velocities2 = df_resampled.diff() / (10)\n",
    "top_cols2 = velocities2.apply(lambda x: x.nlargest(50).index.tolist(), axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
